{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf63e81f-0026-4d25-a824-f5c74e3e9230",
   "metadata": {},
   "source": [
    "# Simple Image Classification using CNN - Deep Learning\n",
    "In this article, I will be solving an image classification problem, where my goal will be to tell which class the input image belongs to. The way I am going to achieve it is by training an artificial neural network on a few thousand images of cats and dogs and making the NN(Neural Network) learn to predict which class the image belongs to, the next time it sees an image having a cat or dog in it.\n",
    "\n",
    "The dataset: https://www.microsoft.com/en-US/download/details.aspx?id=54765\n",
    "\n",
    "Please note that the dataset has to contain 2 files, one for training and the other for testing, each file has to contain 2 sub-files, one for cats and the other for dogs.\n",
    "\n",
    "The process of building a Convolutional Neural Network always involves four major steps.\r\n",
    "- Convolution\n",
    "- Pooling\n",
    "- Flattening\n",
    "- Full connection\n",
    "\n",
    "I will be going through each of the above operations while coding our neural network.nnection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805929a-931a-4d6e-9b20-564e92541b61",
   "metadata": {},
   "source": [
    "### Importing the Keras libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2df0df7-d62f-4798-8644-995b89aa602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c223b8-7519-413c-af76-e745823d76e7",
   "metadata": {},
   "source": [
    "Let us now see what each of the above packages are imported for :\n",
    "\n",
    "- We’ve imported Sequential from keras.models, to initialise our neural network model as a sequential network.\n",
    "  \n",
    "- We’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN, on the training images. Since we are working on images here, which a basically 2 Dimensional arrays, we’re using Convolution 2-D.\n",
    "  \n",
    "- We’ve imported MaxPooling2D from keras.layers, which is used for pooling operation, that is the step — 2 in the process of building a CNN. Here in MaxPooling we need the maximum value pixel from the respective region of interest.\n",
    "  \n",
    "- We’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector.\n",
    "  \n",
    "- we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network, which is the step 4 in the process of building a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb183f-df3f-4241-a041-0201ada61792",
   "metadata": {},
   "source": [
    "### Building the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39319a25-09a7-4bea-bf33-f76df515989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential() # creating an object of Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1665b26d-1248-444a-a5a2-3aa91a885a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding the Convolution step:\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3722f-38fd-4b2c-9a79-b6f53dd42a92",
   "metadata": {},
   "source": [
    "The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image(RGB or Black and White)of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and “3” stands for RGB, which is a colour img, the fourth argument is the activation function we want to use, here ‘relu’ stands for a rectifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "732bf6bb-4f86-48f1-8a93-c6af688b2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pooling operation on the resultant feature after the convolution operation is done.\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22878d-e408-424e-ae05-ea2171dda4a4",
   "metadata": {},
   "source": [
    "The primary aim of a pooling operation is to reduce the size of the images as much as possible. The key thing to understand here is that we are trying to reduce the total number of nodes for the upcoming layers. We take a 2x2 matrix we’ll have minimum pixel loss and get a precise region where the feature are located. We just reduced the complexity of the model without reducing it’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5043fbed-e3ea-402d-8f18-23bf6499c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all the pooled images into a continuous vector\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ae561-da8e-426d-a4cc-6ee3d72ba93d",
   "metadata": {},
   "source": [
    "What we are basically doing here is taking the 2-D array, i.e pooled image pixels and converting them to a one dimensional single vector. We no need to add any special parameters, keras will understand that the “classifier” object is already holding pooled image pixels and they need to be flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "373b2e1e-3814-4476-96eb-04860d2fa68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041b2e1-de6a-416b-8546-e7c2f6d2e561",
   "metadata": {},
   "source": [
    "We are going to connect the set of nodes we got after the flattening step, these nodes will act as an input layer to these fully-connected layers. As this layer will be present between the input layer and output layer, we can refer to it a hidden layer. \n",
    "\n",
    "Dense is the function to add a fully connected layer, ‘units’ is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc71bf65-3f52-49d2-bc0f-aedacefd2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise our output layer\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2171ef-9646-49fd-ab26-6cfedc8b62d9",
   "metadata": {},
   "source": [
    "The output layer should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ba1495d5-889c-4ca8-bc47-53b071fe245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling our CNN model\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb802e21-764b-4d66-9dbe-f35a44004fac",
   "metadata": {},
   "source": [
    "- Optimizer parameter is to choose the stochastic gradient descent algorithm.\n",
    "- Loss parameter is to choose the loss function.\n",
    "- Finally, the metrics parameter is to choose the performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237eaf7b-6973-42f8-955f-1675a0d60916",
   "metadata": {},
   "source": [
    "### Images pre-processing\n",
    "we are going to pre-process the images to prevent over-fitting. Overfitting is when you get a great training accuracy and very poor test accuracy due to overfitting of nodes from one layer to another.\n",
    "\n",
    "So before we fit our images to the neural network, we need to perform some image augmentations on them, which is basically synthesising the training data. The directory’s name is take as the label of all the images present in it. For example : All the images inside the ‘cats’ named folder will be considered as cats by keras.\n",
    "\n",
    "Also, we are creating synthetic data out of the same images by performing different type of operations on these images like flipping, rotating, blurring, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "61a9f30e-51a1-4646-8857-b7150507f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19999 images belonging to 2 classes.\n",
      "Found 4999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'dataset_img\\training_set', \n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'dataset_img\\test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0004fbe-8ff3-4321-adb0-d3c02bc0c8fe",
   "metadata": {},
   "source": [
    "### Fitting the data to our model and making new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca75d35f-e833-4e55-b3cd-3d7c52026cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problematic images found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "# Define the directory containing your dataset\n",
    "dataset_directory = 'dataset_img/training_set'  # Change to your dataset directory\n",
    "\n",
    "# Create a list to store the filenames of problematic images\n",
    "problematic_images = []\n",
    "\n",
    "# Iterate through all image files in the directory\n",
    "for root, dirs, files in os.walk(dataset_directory):\n",
    "    for file in files:\n",
    "        image_path = os.path.join(root, file)\n",
    "        try:\n",
    "            # Attempt to load the image using Keras\n",
    "            load_img(image_path)\n",
    "        except Exception as e:\n",
    "            # If an error occurs, the image is problematic\n",
    "            print(f\"Problematic Image: {image_path}\")\n",
    "            problematic_images.append(image_path)\n",
    "\n",
    "# If problematic images were found, you can choose to delete them\n",
    "if problematic_images:\n",
    "    print(f\"Found {len(problematic_images)} problematic image(s).\")\n",
    "    delete_images = input(\"Do you want to delete these images? (y/n): \").strip().lower()\n",
    "    \n",
    "    if delete_images == 'y':\n",
    "        for image_path in problematic_images:\n",
    "            os.remove(image_path)\n",
    "            print(f\"Deleted: {image_path}\")\n",
    "        print(\"Problematic images deleted.\")\n",
    "    else:\n",
    "        print(\"No images were deleted.\")\n",
    "else:\n",
    "    print(\"No problematic images found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef39cbd-469c-4b90-bb86-f89e8e971407",
   "metadata": {},
   "source": [
    "**I had some corrupted images in the dataset so I used the above code to delete these images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0cd0cd83-6641-4202-ad90-e4b1fb5343da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.3051 - accuracy: 0.8701 - val_loss: 0.7642 - val_accuracy: 0.7188\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 0.2887 - accuracy: 0.8898 - val_loss: 0.5925 - val_accuracy: 0.7812\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2950 - accuracy: 0.8553 - val_loss: 0.4286 - val_accuracy: 0.7891\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.2771 - accuracy: 0.8734 - val_loss: 0.5550 - val_accuracy: 0.8125\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 0.2230 - accuracy: 0.9178 - val_loss: 0.6001 - val_accuracy: 0.7812\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2571 - accuracy: 0.8898 - val_loss: 0.5947 - val_accuracy: 0.7578\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.2837 - accuracy: 0.8651 - val_loss: 0.7311 - val_accuracy: 0.7031\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2680 - accuracy: 0.8898 - val_loss: 0.7735 - val_accuracy: 0.7344\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 0.2555 - accuracy: 0.8882 - val_loss: 0.5640 - val_accuracy: 0.7969\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.2613 - accuracy: 0.8997 - val_loss: 0.5366 - val_accuracy: 0.7656\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2946 - accuracy: 0.8734 - val_loss: 0.4653 - val_accuracy: 0.7969\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.2962 - accuracy: 0.8799 - val_loss: 0.6999 - val_accuracy: 0.7266\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.2871 - accuracy: 0.8635 - val_loss: 0.6006 - val_accuracy: 0.7969\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2821 - accuracy: 0.8684 - val_loss: 0.8663 - val_accuracy: 0.6953\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.2912 - accuracy: 0.8684 - val_loss: 0.5758 - val_accuracy: 0.7812\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2528 - accuracy: 0.8832 - val_loss: 0.6107 - val_accuracy: 0.7969\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2967 - accuracy: 0.8635 - val_loss: 0.5269 - val_accuracy: 0.7969\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2761 - accuracy: 0.8816 - val_loss: 0.6451 - val_accuracy: 0.7578\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 2s 78ms/step - loss: 0.2982 - accuracy: 0.8914 - val_loss: 0.7260 - val_accuracy: 0.7344\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2315 - accuracy: 0.8997 - val_loss: 0.6593 - val_accuracy: 0.7266\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.2804 - accuracy: 0.8783 - val_loss: 0.6530 - val_accuracy: 0.7812\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2921 - accuracy: 0.8799 - val_loss: 0.5677 - val_accuracy: 0.7734\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 2s 78ms/step - loss: 0.2937 - accuracy: 0.8717 - val_loss: 0.5044 - val_accuracy: 0.7500\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.2688 - accuracy: 0.8816 - val_loss: 0.6213 - val_accuracy: 0.7266\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.3233 - accuracy: 0.8602 - val_loss: 0.4849 - val_accuracy: 0.8281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f4dc561c50>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(training_set) // batch_size\n",
    "validation_steps = len(test_set) // batch_size\n",
    "\n",
    "classifier.fit(training_set, \n",
    "               steps_per_epoch = steps_per_epoch, \n",
    "               epochs=25, \n",
    "               validation_data = test_set,\n",
    "               validation_steps = validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476551b-2c84-4e13-833f-0eb019d7b741",
   "metadata": {},
   "source": [
    "steps_per_epoch: This parameter specifies the number of batches of data that the model should process from the training set during each epoch. In other words, it determines how many times the model will update its weights based on the training data within a single epoch. The value of steps_per_epoch is often set based on the total number of training samples and the batch size.\n",
    "\n",
    "validation_steps: This parameter is similar to steps_per_epoch but is used during the validation or testing phase. It specifies the number of batches of data from the validation set that the model should process during each validation epoch. Like steps_per_epoch, the value of validation_steps is often set based on the total number of validation samples and the batch size.\n",
    "\n",
    "\r\n",
    "And ‘epochs’, A single epoch is a single step in training a neural network; in other words when a neural network is trained on every training samples only in one pass we say that one epoch is finished. So training process should consist more than one epochs.In this case we have defined 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "62d6fda4-c037-4778-bc64-621de21ff11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "It's a Dog!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img(r'dataset_img/American_Eskimo_Dog.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "\n",
    "print(f'It\\'s a {prediction}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7708bca-fb32-4de9-8f92-905f40fa8561",
   "metadata": {},
   "source": [
    "The test_image holds the image that needs to be tested on the CNN. Once we have the test image, we will prepare the image to be sent into the model by converting its resolution to 64x64 as the model only excepts that resolution. Then we are using predict() method on our classifier object to get the prediction. As the prediction will be in a binary form, we will be receiving either a 1 or 0, which will represent a dog or a cat respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c5830-66d0-4c12-a8f6-726833a67b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
